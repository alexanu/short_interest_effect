{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug DAGs\n",
    "\n",
    "In this notebook, we debug our DAGs without running them through Apache Airflow, that is, by pasing them to EMR cluster directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('airflow/dags/lib')\n",
    "import emrspark_lib as emrs\n",
    "import configparser\n",
    "import time\n",
    "import boto3\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from datetime import date\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('airflow/config.cfg')\n",
    "\n",
    "CLUSTER_NAME = config['AWS']['CLUSTER_NAME']\n",
    "VPC_ID = config['AWS']['VPC_ID']\n",
    "SUBNET_ID = config['AWS']['SUBNET_ID']\n",
    "\n",
    "today = date.today()\n",
    "PULL_DATE = today.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "if config['App']['STOCKS'] == '':\n",
    "    STOCKS = []\n",
    "else:\n",
    "    STOCKS = json.loads(config.get('App', 'STOCKS').replace(\"'\", '\"'))\n",
    "\n",
    "ec2, emr, iam = emrs.get_boto_clients(config['AWS']['REGION_NAME'], config=config)\n",
    "\n",
    "if VPC_ID == '':\n",
    "    VPC_ID = emrs.get_first_available_vpc(ec2)\n",
    "\n",
    "if SUBNET_ID == '':\n",
    "    SUBNET_ID = emrs.get_first_available_subnet(ec2, VPC_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_sg_id = emrs.create_security_group(ec2, '{}SG'.format(CLUSTER_NAME),\n",
    "    'Master SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "slave_sg_id = emrs.create_security_group(ec2, '{}SlaveSG'.format(CLUSTER_NAME),\n",
    "    'Slave SG for {}'.format(CLUSTER_NAME), VPC_ID)\n",
    "\n",
    "keypair = emrs.create_key_pair(ec2, '{}_pem'.format(CLUSTER_NAME))\n",
    "\n",
    "emrs.create_default_roles(iam)\n",
    "emrs.wait_for_roles(iam)\n",
    "\n",
    "cluster_id = emrs.create_emr_cluster(emr, CLUSTER_NAME,\n",
    "                master_sg_id,\n",
    "                slave_sg_id,\n",
    "                keypair['KeyName'], SUBNET_ID,\n",
    "                release_label='emr-5.28.1',\n",
    "                num_core_nodes=3)\n",
    "cluster_dns = emrs.get_cluster_dns(emr, cluster_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The cells below do not need to be run sequentially.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging with Local Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "#         .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "#         .config(\"spark.eventLog.dir\" \"test_data/spark-logs\") \\\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", config['AWS']['AWS_ACCESS_KEY_ID'])\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", config['AWS']['AWS_SECRET_ACCESS_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv('s3a://short-interest-effect/data/raw/short_interests_nasdaq', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "df = df.withColumn(\"index\", monotonically_increasing_id())\n",
    "\n",
    "# Query with the index\n",
    "tail = sqlContext.sql(\"\"\"SELECT * FROM df ORDER BY index DESC limit 5\"\"\")\n",
    "tail.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging with Spark on Cluster\n",
    "\n",
    "Simply update the arguments and the file in `job_response_headers` initialization to run a code. All files inside the `/debugging` directory should be able to be run, as well as in `/airflow/dags/etl` directory.\n",
    "\n",
    "`sleep_seconds` parameter can also be adjusted accordingly to set how often to get feedback from the Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_debug = {\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'PULL_DATE': PULL_DATE,\n",
    "    'URL_NASDAQ': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download',\n",
    "    'URL_NYSE': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download',\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'LIMIT': None,\n",
    "    'STOCKS': ['BRK_A'],\n",
    "    'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "    'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "    'TABLE_SHORT_ANALYSIS': config['App']['TABLE_SHORT_ANALYSIS_QUANTOPIAN'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        # Update this parameter with the script's path:\n",
    "        'debugging/test-spark_table_exists.py',\n",
    "        args=args_debug,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "# Update the sleep_seconds when needed:\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers, sleep_seconds=10)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Console - end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Stock Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'URL_NASDAQ': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download',\n",
    "    'URL_NYSE': 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download',\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_stock_info.py',\n",
    "        args=args,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_id)\n",
    "print(master_sg_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Short Interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_si = {\n",
    "    'START_DATE': config['App']['START_DATE'],\n",
    "    'QUANDL_API_KEY': config['Quandl']['API_KEY'],\n",
    "    'PULL_DATE': PULL_DATE,\n",
    "#     'LIMIT': config['App']['STOCK_LIMITS'],\n",
    "#     'STOCKS': STOCKS,\n",
    "    'LIMIT': None,\n",
    "    'STOCKS': ['SPY', 'BRK_A'],\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_STOCK_INFO_NASDAQ': config['App']['TABLE_STOCK_INFO_NASDAQ'],\n",
    "    'TABLE_STOCK_INFO_NYSE': config['App']['TABLE_STOCK_INFO_NYSE'],\n",
    "    'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "    'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_short_interests.py',\n",
    "        args=args_si,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers, sleep_seconds=120)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality-check - Short Interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_qs = {\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'PULL_DATE': PULL_DATE,\n",
    "            'STOCKS': None,\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "            'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/pull_short_interests_quality.py',\n",
    "        args=args_qs,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers, sleep_seconds=120)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "20/02/06 04:22:11 WARN DAG: NASDAQ short interest latest date: 2020-02-03\n",
    "20/02/06 04:25:16 WARN DAG: Last 5 dates: [Row(Date=u'2020-02-03'), Row(Date=u'2020-02-03'), Row(Date=u'2020-02-03'), Row(Date=u'2020-02-03'), Row(Date=u'2020-02-03')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Data\n",
    "\n",
    "And adjust for use in Quantopian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_c = {\n",
    "            'PULL_DATE': PULL_DATE,\n",
    "            'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "            'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "            'DB_HOST': config['App']['DB_HOST'],\n",
    "            'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "            'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "            'TABLE_SHORT_ANALYSIS': config['App']['TABLE_SHORT_ANALYSIS_QUANTOPIAN'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/combine.py',\n",
    "        args=args_c,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers, sleep_seconds=120)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality-check - Combine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_qc = {\n",
    "    'AWS_ACCESS_KEY_ID': config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "    'AWS_SECRET_ACCESS_KEY': config['AWS']['AWS_SECRET_ACCESS_KEY'],\n",
    "    'PULL_DATE': PULL_DATE,\n",
    "    'STOCKS': None,\n",
    "    'DB_HOST': config['App']['DB_HOST'],\n",
    "    'TABLE_SHORT_INTERESTS_NASDAQ': config['App']['TABLE_SHORT_INTERESTS_NASDAQ'],\n",
    "    'TABLE_SHORT_INTERESTS_NYSE': config['App']['TABLE_SHORT_INTERESTS_NYSE'],\n",
    "    'TABLE_SHORT_ANALYSIS': config['App']['TABLE_SHORT_ANALYSIS_QUANTOPIAN'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(logging.INFO)\n",
    "emrs.kill_all_spark_sessions(cluster_dns)\n",
    "session_headers = emrs.create_spark_session(cluster_dns)\n",
    "emrs.wait_for_spark(cluster_dns, session_headers)\n",
    "job_response_headers = emrs.submit_spark_job_from_file(\n",
    "        cluster_dns, session_headers,\n",
    "        'airflow/dags/etl/combine_quality.py',\n",
    "        args=args_qc,\n",
    "        commonpath='airflow/dags/etl/common.py',\n",
    "        helperspath='airflow/dags/etl/helpers.py'\n",
    ")\n",
    "final_status, logs = emrs.track_spark_job(cluster_dns, job_response_headers, sleep_seconds=10)\n",
    "emrs.kill_spark_session(cluster_dns, session_headers)\n",
    "for line in logs:\n",
    "    logging.info(line)\n",
    "    if '(FAIL)' in str(line):\n",
    "        logging.error(line)\n",
    "        raise Exception(\"ETL process fails.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Touch\n",
    "\n",
    "Update files to public-readable ARN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = config['App']['DB_HOST'].split('/')[-1]\n",
    "\n",
    "key = config['App']['TABLE_SHORT_ANALYSIS_QUANTOPIAN'][1:]+'.csv'\n",
    "(boto3\n",
    " .session\n",
    " .Session(region_name='us-east-1')\n",
    " .resource('s3')\n",
    " .Object(bucket, key)\n",
    " .copy_from(CopySource={'Bucket': bucket,\n",
    "                        'Key': key},\n",
    "            MetadataDirective=\"REPLACE\",\n",
    "            ContentType=\"text/csv\")\n",
    ")\n",
    "(boto3\n",
    " .session\n",
    " .Session(region_name='us-east-1')\n",
    " .resource('s3')\n",
    " .Object(bucket, key)\n",
    " .Acl()\n",
    " .put(ACL='public-read'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emrs.delete_cluster(emr, cluster_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
