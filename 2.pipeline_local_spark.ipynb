{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Pipeline on Local Spark\n",
    "\n",
    "1. Setting up\n",
    "2. Helpers\n",
    "3. Pull stock info\n",
    "4. Pull short interests\n",
    "5. Pull stock prices\n",
    "6. Combine datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up\n",
    "\n",
    "We want to have a logging feature that works for both Jupyter notebook and Spark environments.\n",
    "\n",
    "1. As it turned out, Spark has \"WARN\" but does not have \"WARNING\" level, while in current Python (3.6.x), \"WARN\" is deprecated, \"WARNING\" should be used instead.\n",
    "2. Therefore, we create a custom \"WARN\" level as well as function `logger.warn` for Jupyter notebook.\n",
    "3. As shown in [this StackOverflow post](https://stackoverflow.com/questions/35326814/change-level-logged-to-ipython-jupyter-notebook), this is not straightforward due to a Jupyter notebook bug. We need to workaround this by specifying an invalid value first, which we do in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run this, but don't copy into etl scripts\n",
    "# workaround via specifying an invalid value first\n",
    "%config Application.log_level='WORKAROUND'\n",
    "import logging\n",
    "logging.WARN = 21\n",
    "logging.addLevelName(logging.WARN, 'WARN')\n",
    "\n",
    "def warn(self, message, *args, **kws):\n",
    "    if self.isEnabledFor(logging.WARN):\n",
    "        # Yes, logger takes its '*args' as 'args'.\n",
    "        self._log(logging.WARN, message, args, **kws) \n",
    "logging.Logger.warn = warn\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARN)\n",
    "logger.warn('hello')\n",
    "\n",
    "# ------------------\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "#         .config(\"spark.eventLog.enabled\", \"true\") \\\n",
    "#         .config(\"spark.eventLog.dir\" \"test_data/spark-logs\") \\\n",
    "\n",
    "import pandas as pd\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('airflow/config.cfg')\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell, and also copy to all etl scripts, or simply include in common.py\n",
    "\n",
    "import requests\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from py4j.java_gateway import java_import"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Copy this cell content to common.py, but don't run here.\n",
    "\n",
    "Logger = spark._jvm.org.apache.log4j.Logger\n",
    "logger = Logger.getLogger(\"DAG\")\n",
    "spark.sparkContext.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helpers\n",
    "\n",
    "Include this code as helpers in all next etl scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
    "\n",
    "\n",
    "def delete_path(spark, host, path):\n",
    "    sc = spark.sparkContext\n",
    "    java_import(sc._gateway.jvm, \"java.net.URI\")\n",
    "    uri = sc._gateway.jvm.java.net.URI\n",
    "    fs = (sc._jvm.org\n",
    "          .apache.hadoop\n",
    "          .fs.FileSystem\n",
    "          .get(uri(host), sc._jsc.hadoopConfiguration())\n",
    "          )\n",
    "    fs.delete(sc._jvm.org.apache.hadoop.fs.Path(host+path), True)\n",
    "\n",
    "\n",
    "def copyMerge(spark, host, src_dir, dst_file, overwrite=False, deleteSource=False, debug=False):\n",
    "    \n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    hadoop = sc._jvm.org.apache.hadoop\n",
    "#     conf = hadoop.conf.Configuration()\n",
    "    conf = sc._jsc.hadoopConfiguration()\n",
    "#     fs = hadoop.fs.FileSystem.get(conf)\n",
    "    java_import(sc._gateway.jvm, \"java.net.URI\")\n",
    "    uri = sc._gateway.jvm.java.net.URI\n",
    "    fs = (sc._jvm.org\n",
    "          .apache.hadoop\n",
    "          .fs.FileSystem\n",
    "          .get(uri(host), sc._jsc.hadoopConfiguration())\n",
    "          )\n",
    "\n",
    "    # check files that will be merged\n",
    "    files = []\n",
    "    for f in fs.listStatus(hadoop.fs.Path(src_dir)):\n",
    "        if f.isFile():\n",
    "            files.append(f.getPath())\n",
    "    if not files:\n",
    "        raise ValueError(\"Source directory {} is empty\".format(src_dir))\n",
    "    files.sort(key=lambda f: str(f))\n",
    "\n",
    "    # dst_permission = hadoop.fs.permission.FsPermission.valueOf(permission)      # , permission='-rw-r-----'\n",
    "    out_stream = fs.create(hadoop.fs.Path(dst_file), overwrite)\n",
    "\n",
    "    try:\n",
    "        # loop over files in alphabetical order and append them one by one to the target file\n",
    "        for file in files:\n",
    "            if debug: \n",
    "                print(\"Appending file {} into {}\".format(file, dst_file))\n",
    "\n",
    "            in_stream = fs.open(file)   # InputStream object\n",
    "            try:\n",
    "                hadoop.io.IOUtils.copyBytes(in_stream, out_stream, conf, False)     # False means don't close out_stream\n",
    "            finally:\n",
    "                in_stream.close()\n",
    "    finally:\n",
    "        out_stream.close()\n",
    "\n",
    "    if deleteSource:\n",
    "        fs.delete(hadoop.fs.Path(src_dir), True)    # True=recursive\n",
    "        if debug:\n",
    "            print(\"Source directory {} removed.\".format(src_dir))\n",
    "\n",
    "\n",
    "def spark_table_exists(host, table_path):\n",
    "    URI           = sc._gateway.jvm.java.net.URI\n",
    "    Path          = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
    "    FileSystem    = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
    "    # Configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
    "    Configuration = sc._jsc.hadoopConfiguration\n",
    "\n",
    "\n",
    "    fs = FileSystem.get(URI(host), Configuration())\n",
    "\n",
    "    try:\n",
    "        status = fs.listStatus(Path(table_path))\n",
    "        spark.read.csv(host+table_path, header=True)\n",
    "\n",
    "        return True\n",
    "    except Py4JJavaError as e:\n",
    "        if 'FileNotFoundException' in str(e):\n",
    "            return False\n",
    "        else:\n",
    "            raise\n",
    "    except AnalysisException as e:\n",
    "        if 'Unable to infer schema' in str(e):\n",
    "            return False\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def check_basic_quality(logger, host, table_path, table_type='csv'):\n",
    "    \"\"\" Checks quality of DAG.\n",
    "    \n",
    "    We do this by checking if the table exists and is not empty.\n",
    "    \n",
    "    Args:\n",
    "        - table_type(str): 'parquet' or 'csv'\n",
    "    \"\"\"\n",
    "    if not spark_table_exists(host, table_path):\n",
    "        logger.warn(\"(FAIL) Table {} does not exist\".format(host+table_path))\n",
    "        return None\n",
    "    else:\n",
    "        if table_type == 'parquet':\n",
    "            sdf = spark.read.parquet(host+table_path)\n",
    "            count = sdf.rdd.countApprox(timeout=1000, confidence=0.9)\n",
    "        elif table_type == 'csv':\n",
    "            sdf = spark.read.csv(host+table_path, header=True)\n",
    "            count = sdf.rdd.countApprox(timeout=1000, confidence=0.9)\n",
    "            \n",
    "        if count == 0:\n",
    "            logger.warn(\"(FAIL) Table {} is empty.\".format(host+table_path))\n",
    "        else:\n",
    "            logger.warn(\"(SUCCESS) Table {} has {} rows.\".format(host+table_path, count))\n",
    "        return sdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark_table_exists('s3a://short-interest-effect', 'data/test_table')) # Fails due to lack of '/' before the table path\n",
    "print(spark_table_exists('s3a://short-interest-effect', '/data/test_table'))\n",
    "print(spark_table_exists('', 'test_data/test_table'))\n",
    "\n",
    "check_basic_quality(logger, 's3a://short-interest-effect', '/data/test_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pull Stock Info\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass into `args` argument\n",
    "\n",
    "URL_NASDAQ = 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nasdaq&render=download'\n",
    "URL_NYSE = 'https://old.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=nyse&render=download'\n",
    "\n",
    "DB_HOST = ''\n",
    "# Table names: update to add '/' in the final code.\n",
    "TABLE_STOCK_INFO_NASDAQ = 'test_data/raw/stock_info_nasdaq'\n",
    "TABLE_STOCK_INFO_NYSE = 'test_data/raw/stock_info_nyse'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The EMR cluster does not have pandas so we cannot use this.\n",
    "import pandas as pd\n",
    "\n",
    "def pull_stock_info(url, table_path):\n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        df['MarketCap'] = df['MarketCap'].astype(str)\n",
    "        df['Sector'] = df['Sector'].astype(str)\n",
    "        df['industry'] = df['industry'].astype(str)\n",
    "        spark.createDataFrame(df) \\\n",
    "            .withColumnRenamed('Summary Quote', 'SummaryQuote') \\\n",
    "            .withColumnRenamed('Unnamed: 8', '_c8') \\\n",
    "            .drop('_c8') \\\n",
    "            .write.mode('overwrite').parquet(table_path)\n",
    "        logger.warn(\"Stored data from {} to {}\".format(url, table_path))\n",
    "    except IOError as e:\n",
    "        logger.warn(\"Failed to connect to {}. We will use existing stock info data if they have been created.\".format(url))\n",
    "        \n",
    "    \n",
    "pull_stock_info(URL_NASDAQ, DB_HOST+TABLE_STOCK_INFO_NASDAQ)\n",
    "pull_stock_info(URL_NYSE, DB_HOST+TABLE_STOCK_INFO_NYSE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# On the EMR cluster, this code returns an error:\n",
    "# \"AttributeError: 'RDD' object has no attribute '_get_object_id'\\n\"\n",
    "# quite likely due to loading CSV from string: spark.read.csv(data, header=True)\n",
    "\n",
    "def pull_stock_info(url, table_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200 or response.status_code == 201:\n",
    "        content = response.content.decode('utf-8')\n",
    "        data = spark.sparkContext.parallelize(content.splitlines())\n",
    "        logger.warn(\"data is {}\".format(data))\n",
    "        df = spark.read.csv(data, header=True) \\\n",
    "            .withColumnRenamed('Summary Quote', 'SummaryQuote') \\\n",
    "            .drop('_c8') \\\n",
    "            .write.mode('overwrite').parquet(table_path)\n",
    "        logger.warn(\"Stored data from {} to {}\".format(url, table_path))\n",
    "    else:\n",
    "        logger.warn(\"Failed to connect to {}. We will use existing stock info data if they have been created.\".format(url))\n",
    "        \n",
    "    \n",
    "pull_stock_info(URL_NASDAQ, DB_HOST+TABLE_STOCK_INFO_NASDAQ)\n",
    "pull_stock_info(URL_NYSE, DB_HOST+TABLE_STOCK_INFO_NYSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_stock_info(url, db_host, table_path):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200 or response.status_code == 201:\n",
    "        content = response.content.decode('utf-8')\n",
    "        content = content.replace('Summary Quote', 'SummaryQuote')\n",
    "        delete_path(spark, db_host, table_path)\n",
    "        df = spark.createDataFrame([[content]], ['info_csv'])\n",
    "        df.rdd.map(lambda x: x['info_csv'].replace(\"[\",\"\").replace(\"]\", \"\")).saveAsTextFile(db_host+table_path)\n",
    "        logger.warn(\"Stored data from {} to {}\".format(url, db_host+table_path))\n",
    "    else:\n",
    "        logger.warn(\"Failed to connect to {}. We will use existing stock info data if they have been created.\".format(url))\n",
    "        \n",
    "    \n",
    "pull_stock_info(URL_NASDAQ, DB_HOST, TABLE_STOCK_INFO_NASDAQ)\n",
    "pull_stock_info(URL_NYSE, DB_HOST, TABLE_STOCK_INFO_NYSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB_HOST = 's3a://short-interest-effect'\n",
    "# TABLE_STOCK_INFO_NASDAQ = '/data/raw/stock_info_nasdaq'\n",
    "\n",
    "df = spark.read.csv(DB_HOST+TABLE_STOCK_INFO_NASDAQ, header=True) \\\n",
    "               .drop('_c8').toPandas()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(DB_HOST+TABLE_STOCK_INFO_NYSE,\n",
    "                    header=True, ignoreLeadingWhiteSpace=True, inferSchema=True) \\\n",
    "               .drop('_c8').toPandas()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_basic_quality(logger, DB_HOST, TABLE_STOCK_INFO_NASDAQ, table_type='csv')\n",
    "check_basic_quality(logger, DB_HOST, TABLE_STOCK_INFO_NYSE, table_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pull Short Interest\n",
    "\n",
    "#### Parallelize based on stocks or parallelize based on returned data points?\n",
    "\n",
    "At the time of writing (2020-01-15), we have 3582 stocks from NASDAQ and 3092 stocks from NYSE. The earliest date is 2013-04-01, which accounts for nearly 1700 data points (261 working days each year).\n",
    "\n",
    "For each stock, we will need to connect to an external API (Quandl or QuoteMedia). This will take more of the processing time rather than data processing. Therefore, we parallelize based on the stocks rather than returned data points. This way, multiple Spark nodes can connect to different URLs and pull the data. The downside is, obviously, for each node we will have to iteratively process the data, but this is still faster as there are fewer data points than the stocks, at least until several years in the future (There might be a solution that allows each spark node to parallelize...).\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exchange_map = {\n",
    "#     'nasdaq': 'FNSQ',\n",
    "#     'nyse': 'FNYX'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.quandl.com/api/v3/datasets/FINRA/FNYX_FB?api_key={}\".format(config['Quandl']['API_KEY'])\n",
    "result = requests.get(url).json()\n",
    "print(result['dataset']['data'][0])\n",
    "print(result['dataset']['column_names'])\n",
    "col_names = [result['dataset']['column_names']] * len(result['dataset']['data'])\n",
    "newdata = []\n",
    "for i, cols in enumerate(col_names):\n",
    "    newdata.append(dict(zip(cols, result['dataset']['data'][i])))\n",
    "newdata[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to single tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass into `args` argument\n",
    "\n",
    "START_DATE = config['App']['START_DATE']\n",
    "QUANDL_API_KEY = config['Quandl']['API_KEY']\n",
    "YESTERDAY_DATE = '2019-12-16'\n",
    "LIMIT = 100\n",
    "# STOCKS = ['FB', 'GOOG', 'AMZN', 'TRMT', 'TSLA', 'MCD', 'NFLX']\n",
    "STOCKS = []\n",
    "AWS_ACCESS_KEY_ID = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "DB_HOST = ''\n",
    "\n",
    "# Table names: update to add '/' in the final code.\n",
    "TABLE_STOCK_INFO_NASDAQ = 'test_data/raw/stock_info_nasdaq'\n",
    "TABLE_STOCK_INFO_NYSE = 'test_data/raw/stock_info_nyse'\n",
    "TABLE_SHORT_INTERESTS_NASDAQ = 'test_data/raw/short_interests_nasdaq' \n",
    "TABLE_SHORT_INTERESTS_NYSE = 'test_data/raw/short_interests_nyse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def a_before_b(a, b):\n",
    "    date_format = \"%Y-%m-%d\"\n",
    "\n",
    "    # create datetime objects from the strings\n",
    "    da = datetime.strptime(a, date_format)\n",
    "    db = datetime.strptime(b, date_format)\n",
    "\n",
    "    if da < db:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def rowlist2dict(rowlist):\n",
    "    obj = {}\n",
    "    for row in rowlist:\n",
    "        obj[row['Symbol']] = row['last_date']\n",
    "    return obj\n",
    "\n",
    "\n",
    "def convert_data(olddata, symbol, url):\n",
    "    col_names = olddata['dataset']['column_names']\n",
    "    col_names.append('Symbol')\n",
    "    col_names.append('SourceURL')\n",
    "    col_names_multiplied = [col_names] * len(olddata['dataset']['data'])\n",
    "    newdata = []\n",
    "    for i, cols in enumerate(col_names_multiplied):\n",
    "        datum = olddata['dataset']['data'][i]\n",
    "        datum.append(symbol)\n",
    "        datum.append(url)\n",
    "        newdata.append(Row(**dict(zip(cols, datum))))\n",
    "    return newdata\n",
    "\n",
    "\n",
    "def pull_short_interests(exchange, host, info_table_path, short_interests_table_path, log_every_n=100):\n",
    "        \n",
    "    def pull_exchange_short_interests_by_symbol(symbol, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            list of dicts [{'colname': value, ...}, ...]\n",
    "        \"\"\"\n",
    "        url = 'https://www.quandl.com/api/v3/datasets/FINRA/'+exchange+'_{}?start_date='+start_date+'&end_date='+end_date+'&api_key='+QUANDL_API_KEY\n",
    "        url = url.format(symbol)\n",
    "        response = requests.get(url)\n",
    "        newdata = []\n",
    "        if response.status_code in [200, 201]:\n",
    "            newdata = convert_data(response.json(), symbol, url)\n",
    "        return newdata\n",
    "\n",
    "    # Prepare list of stocks\n",
    "    if STOCKS is not None and len(STOCKS) > 0:\n",
    "        rdd1 = spark.sparkContext.parallelize(STOCKS)\n",
    "        row_rdd = rdd1.map(lambda x: Row(x))\n",
    "        df = spark.createDataFrame(row_rdd,['Symbol'])\n",
    "    else:\n",
    "        df = spark.read.csv(host+info_table_path, header=True)\n",
    "        if LIMIT is not None:\n",
    "            df = df.limit(LIMIT)\n",
    "    symbols = df.select('Symbol').rdd.map(lambda r: r['Symbol']).collect()\n",
    "    \n",
    "    table_exists = spark_table_exists(host, short_interests_table_path)\n",
    "\n",
    "    last_dates = None\n",
    "    if table_exists:\n",
    "        short_sdf = spark.read.csv(host+short_interests_table_path, header=True)\n",
    "        last_dates = short_sdf.groupBy('Symbol').agg(F.max('Date').alias('last_date')).collect()\n",
    "        last_dates = rowlist2dict(last_dates)\n",
    "        \n",
    "    total_rows = 0\n",
    "    data_to_write = []\n",
    "    for i, symbol in enumerate(symbols):\n",
    "        data = []\n",
    "        if table_exists:\n",
    "            # Get the last date of a stock. If this last date >= PULL_DATE, don't do anything.\n",
    "            if last_dates != None:\n",
    "                if symbol in last_dates:\n",
    "                    date = last_dates[symbol]\n",
    "                    if a_before_b(date, PULL_DATE):\n",
    "                        data = pull_exchange_short_interests_by_symbol(symbol, date, PULL_DATE)\n",
    "                        if len(data)==0:\n",
    "                            logger.warn(\"{}: last date ({}) is > pull date ({}) and data exist Keep the data for storing\".format(symbol, date, PULL_DATE))\n",
    "                        else:\n",
    "                            logger.warn(\"{}: last date ({}) is > pull date ({}) but no data is available in Quandl\".format(symbol, date, PULL_DATE))\n",
    "                    else:\n",
    "                        logger.warn(\"{}: last date ({}) is <= pull date ({}), so do nothing\".format(symbol, date, PULL_DATE))\n",
    "                else:\n",
    "                    logger.warn(\"{}: pull data from all dates\".format(symbol))\n",
    "                    data = pull_exchange_short_interests_by_symbol(symbol, START_DATE, PULL_DATE)\n",
    "            else:\n",
    "                logger.warn(\"{}: pull data from all dates\".format(symbol))\n",
    "                data = pull_exchange_short_interests_by_symbol(symbol, START_DATE, PULL_DATE)\n",
    "        else:\n",
    "            data = pull_exchange_short_interests_by_symbol(symbol, START_DATE, PULL_DATE)\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            data_to_write += data\n",
    "\n",
    "        total_rows += len(data)\n",
    "        if (i%log_every_n == 0 or (i+1) == len(symbols)):\n",
    "            logger.warn(\"storing data downloaded from exchange {} - {}/{} - total rows in this batch: {}\".format(exchange, i+1, len(symbols), total_rows))\n",
    "            if len(data_to_write) > 0:\n",
    "                sdf_to_write = spark.createDataFrame(data_to_write)\n",
    "                sdf_to_write.write.mode('append').format('csv').save(host+short_interests_table_path, header=True)\n",
    "                logger.warn(\"Written {} rows to {}\".format(len(data_to_write), host+short_interests_table_path))\n",
    "                data_to_write = []\n",
    "\n",
    "\n",
    "    logger.warn(\"done!\")\n",
    "\n",
    "pull_short_interests('FNSQ', DB_HOST, TABLE_STOCK_INFO_NASDAQ, TABLE_SHORT_INTERESTS_NASDAQ)\n",
    "pull_short_interests('FNYX', DB_HOST, TABLE_STOCK_INFO_NYSE, TABLE_SHORT_INTERESTS_NYSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv(DB_HOST+TABLE_SHORT_INTERESTS_NASDAQ, header=True).limit(5).dropDuplicates(['Date', 'Symbol'])\n",
    "sdf = sdf.orderBy(sdf.Date.desc())\n",
    "print(sdf.count())\n",
    "df = sdf.toPandas()\n",
    "print(df['SourceURL'][0])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['Date', 'Symbol'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass into `args` argument\n",
    "\n",
    "STOCKS = ['FB', 'GOOG', 'AMZN', 'TRMT', 'TSLA', 'MCD', 'NFLX']\n",
    "DB_HOST = ''\n",
    "\n",
    "# Table names: update to add '/' in the final code.\n",
    "TABLE_STOCK_INFO_NASDAQ = 'test_data/raw/stock_info_nasdaq'\n",
    "TABLE_STOCK_INFO_NYSE = 'test_data/raw/stock_info_nyse'\n",
    "TABLE_SHORT_INTERESTS_NASDAQ = 'test_data/raw/short_interests_nasdaq' \n",
    "TABLE_SHORT_INTERESTS_NYSE = 'test_data/raw/short_interests_nyse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STOCKS is None or len(STOCKS) == 0:\n",
    "    check_basic_quality(logger, DB_HOST, TABLE_STOCK_INFO_NASDAQ, table_type='csv')\n",
    "    check_basic_quality(logger, DB_HOST, TABLE_STOCK_INFO_NYSE, table_type='csv')\n",
    "check_basic_quality(logger, DB_HOST, TABLE_SHORT_INTERESTS_NASDAQ)\n",
    "check_basic_quality(logger, DB_HOST, TABLE_SHORT_INTERESTS_NYSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_temp = spark.read.csv(DB_HOST+TABLE_STOCK_PRICES+'-temp', header=True, ignoreLeadingWhiteSpace=True, inferSchema=True).toPandas()\n",
    "print(pdf_temp.info())\n",
    "pdf_temp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = spark.read.csv(DB_HOST+TABLE_STOCK_PRICES, header=True, inferSchema=True) \\\n",
    "    .dropDuplicates(['date', 'symbol']).toPandas()\n",
    "print(pdf.info())\n",
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.sort_values(by=['date', 'symbol'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass into `args` argument\n",
    "\n",
    "STOCKS = ['FB', 'GOOG', 'AMZN', 'TRMT', 'TSLA', 'MCD', 'NFLX']\n",
    "DB_HOST = ''\n",
    "\n",
    "# Table names: update to add '/' in the final code.\n",
    "TABLE_STOCK_INFO_NASDAQ = 'test_data/raw/stock_info_nasdaq'\n",
    "TABLE_STOCK_INFO_NYSE = 'test_data/raw/stock_info_nyse'\n",
    "TABLE_STOCK_PRICES = 'test_data/raw/prices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if STOCKS is None or len(STOCKS) == 0:\n",
    "    check_basic_quality(logger, DB_HOST, TABLE_STOCK_INFO_NASDAQ)\n",
    "    check_basic_quality(logger, DB_HOST, TABLE_STOCK_INFO_NYSE)\n",
    "check_basic_quality(logger, DB_HOST, TABLE_STOCK_PRICES, table_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combine Datasets\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass into `args` argument\n",
    "\n",
    "YESTERDAY_DATE = '2019-12-12'\n",
    "AWS_ACCESS_KEY_ID = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "AWS_SECRET_ACCESS_KEY = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "DB_HOST = ''\n",
    "\n",
    "# Table names: update to add '/' in the final code.\n",
    "TABLE_STOCK_PRICES = 'test_data/raw/prices'\n",
    "TABLE_SHORT_INTERESTS_NASDAQ = 'test_data/raw/short_interests_nasdaq' \n",
    "TABLE_SHORT_INTERESTS_NYSE = 'test_data/raw/short_interests_nyse'\n",
    "TABLE_SHORT_ANALYSIS = 'test_data/processed/short_analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Combine short interest tables\n",
    "# ----------------\n",
    "\n",
    "# Without a schema (even with inferSchema=True), the SUM aggregation fails.\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"Date\", T.StringType(), True),\n",
    "    T.StructField(\"ShortExemptVolume\", T.FloatType(), True),\n",
    "    T.StructField(\"ShortVolume\", T.FloatType(), True),\n",
    "    T.StructField(\"SourceURL\", T.StringType(), True),\n",
    "    T.StructField(\"Symbol\", T.StringType(), True),\n",
    "    T.StructField(\"TotalVolume\", T.FloatType(), True),\n",
    "])\n",
    "\n",
    "sdf_shorts = spark.read.format('csv') \\\n",
    "                  .option('header', True) \\\n",
    "                  .option('schema', schema) \\\n",
    "                  .option('mode', 'DROPMALFORMED') \\\n",
    "                  .load([DB_HOST+TABLE_SHORT_INTERESTS_NASDAQ, DB_HOST+TABLE_SHORT_INTERESTS_NYSE])\n",
    "\n",
    "rows = sdf_shorts.where((F.col('Symbol') == 'SPY') & (F.col('Date') == '2020-02-21')).collect()\n",
    "logger.warn(\"Rows: {}\".format(rows))\n",
    "\n",
    "sdf_shorts = sdf_shorts.groupBy('Date', 'Symbol') \\\n",
    "                 .agg(F.sum('ShortExemptVolume').alias('short_exempt_volume'),\n",
    "                      F.sum('ShortVolume').alias('short_volume'),\n",
    "                      F.sum('TotalVolume').alias('total_volume')\n",
    "                     ) \\\n",
    "                 .withColumnRenamed('Date', 'date') \\\n",
    "                 .withColumnRenamed('Symbol', 'symbol')\n",
    "\n",
    "# ----------------\n",
    "# Prepare for Quantopian\n",
    "# ----------------\n",
    "\n",
    "# DataFrame[short_exempt_volume: string, short_volume: string, total_volume: string, date: string, open: string, \n",
    "# high: string, low: string, close: string, volume: string, changed: string, changep: string, adjclose: string, \n",
    "# tradeval: string, tradevol: string, symbol: string]\n",
    "\n",
    "# Correct all Quantopian errors here.\n",
    "# -----------\n",
    "sdf = sdf_shorts.withColumn('symbol', F.when(F.col('symbol')=='GECCL', 'GECC_L').otherwise(F.col('symbol')))\n",
    "# -----------\n",
    "\n",
    "sdf.select(['date', 'symbol', 'short_exempt_volume', 'short_volume', 'total_volume']) \\\n",
    "   .coalesce(1).write.mode('overwrite').csv(DB_HOST+TABLE_SHORT_ANALYSIS, header=True)\n",
    "\n",
    "delete_path(spark, DB_HOST, TABLE_SHORT_ANALYSIS+\".csv\")\n",
    "copyMerge(spark, DB_HOST, DB_HOST+TABLE_SHORT_ANALYSIS, DB_HOST+TABLE_SHORT_ANALYSIS+\".csv\")\n",
    "delete_path(spark, DB_HOST, TABLE_SHORT_ANALYSIS)\n",
    "\n",
    "logger.warn(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short = sdf_shorts.toPandas()\n",
    "df_prices = sdf_prices.toPandas()\n",
    "df_short_analysis = sdf_short_analysis.toPandas()\n",
    "\n",
    "print(df_short.info())\n",
    "print(df_prices.info())\n",
    "print(df_short_analysis.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(DB_HOST+TABLE_SHORT_ANALYSIS).toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(DB_HOST+TABLE_SHORT_ANALYSIS).repartition(1).write.mode('overwrite').format('csv').save(DB_HOST+TABLE_SHORT_ANALYSIS+\".csv\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass into `args` argument\n",
    "\n",
    "DB_HOST = ''\n",
    "\n",
    "# Table names: update to add '/' in the final code.\n",
    "TABLE_SHORT_ANALYSIS = 'test_data/processed/short_analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_basic_quality(logger, DB_HOST, TABLE_SHORT_ANALYSIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Best practices: https://towardsdatascience.com/apache-airflow-tips-and-best-practices-ff64ce92ef8\n",
    "- Write/store dataframe as textfile: https://stackoverflow.com/questions/44537889/write-store-dataframe-in-text-file\n",
    "- Delete hdfs path: https://stackoverflow.com/a/55952480/278191\n",
    "- Delete hdfs path in S3: http://bigdatatech.taleia.software/2015/12/28/deleting-a-amazon-s3-path-from-apache-spark/\n",
    "- Import java class in python: https://stackoverflow.com/questions/33544105/running-custom-java-class-in-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
